# fun-transformer
本课程涵盖了Transformer的核心组成部分，包含Transformer编码器和解码器的实现。
课程结合代码解读知识点，同时通过形象生动的例子，为学习者理解Transformer模型提供参考和借鉴。
实践项目无需依赖任何深度学习框架，完全从零开始，使用基础的Numpy等科学计算库实现Transformer，旨在深化学习者对模型本质的理解与掌握。
最后，使用Transformer模型实现在NLP任务中的应用，加深对模型的理解


## 课程大纲
| 章节  | 内容 | 代码实现|
| ------------- | ------------- |------------- |
| 第一章 | 引言 |词嵌入|
| 第二章 | Transformer简述 |   相对位置向量  |
| 第三章 | Encoder结构   | 交叉注意力     |
|第四章   |Decoder结构| bert、gpt      |
|第五章 |项目实践| 项目案例、使用Numpy实现Transformer|

## 目录
第一章 引言
- 1. 序列到序列（Seq2Seq）模型概述
- 2. Encoder-Decoder模型概述
- 3. Attention 的提出与影响

第二章 Transformer简述
- 1. Attention 机制
- 2. Transformer概述
- 3. Transformer vs CNN vs RNN
- 4. 输入嵌入(Input Embedding)

第三章 Encoder结构
-  1. 编码器(Encoder)
- 2. 多头自注意力(Multi-Head Self-Attention)
- 3. 交叉自注意力(Cross Attention)
- 4. Cross Attention 和 Self Attention 主要的区别

第四章 Decoder结构
- 1. 解码器(Decoder)
- 2. 掩码(Mask)
- 3. 模型的训练与评估
- 4. 高级主题和应用
  
第五章 Project
- 1. 项目案例
- 2. 使用Numpy实现Transformer


## 参与贡献

- 如果你想参与到项目中来欢迎查看项目的 [Issue]() 查看没有被分配的任务。
- 如果你发现了一些问题，欢迎在 [Issue]() 中进行反馈🐛。
- 如果你对本项目感兴趣想要参与进来可以通过 [Discussion]() 进行交流💬。

如果你对 Datawhale 很感兴趣并想要发起一个新的项目，欢迎查看 [Datawhale 贡献指南](https://github.com/datawhalechina/DOPMC#%E4%B8%BA-datawhale-%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE)。

## 贡献者名单

| 姓名 | 职责 | 简介 |
| :----| :---- | :---- |
| 罗清泉 | 项目负责人 |  |



## 关注我们

<div align=center>
<p>扫描下方二维码关注公众号：Datawhale</p>
<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/qrcode.jpeg" width = "180" height = "180">
</div>

## LICENSE

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。

*注：默认使用CC 4.0协议，也可根据自身项目情况选用其他协议*
