
# 1. 序列到序列（Seq2Seq）模型概述
## 1.1 什么是Seq2Seq模型
Seq2Seq模型，全称Sequence to Sequence模型，就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。


![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%EF%BC%881%EF%BC%89.PNG)

```
<bos>：Begin Of Sequence（BOS）
<eos>：End Of Sequence（EOS）
“BOS I have a pen EOS”，在这里，BOS 和 EOS 分别标记句子的开始和结束。
```  

如上图，输入了**5**个汉字，输出了**4**个英文单词。输入和输出的长度不同。在处理可变长度的序列时，使用 BOS 和 EOS 可以减少对 **填充**（**padding**）的依赖，因为模型可以根据这些标记来识别序列的边界。
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%EF%BC%882%EF%BC%89.png)
    
Seq2Seq模型在概念上还与通信原理有一定的相似性，特别是在信息编码、传输和解码的过程中。

## 1.2 Seq2Seq 的由来
在 Seq2Seq 框架提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在其擅长解决的问题中，输入和输出通常都可以表示为固定长度的向量，如果长度稍有变化，会使用**填充**（**Padding**）等操作，其目的是将序列数据转换成固定长度的格式，以便于输入到需要固定长度输入的神经网络中。

**举个例子**
    
假设我们有一个包含单词索引的句子序列，目标长度为5：<p>
原始序列：**[1, 2, 3]**（长度为3）<p>
Padding后的序列：**[1, 2, 3, 0, 0]**（长度为5，其中0是Padding的元素）<p>

**Padding可能引入的问题**<p>
例如在计算损失时可能需要特殊的处理来忽略这些补零的元素，以免它们影响模型的训练效果。这通常通过使用掩码（Mask）来实现，掩码可以指示哪些位置是补零的，以便在计算过程中忽略这些位置。<p>
然而许多重要的问题，例如机器翻译、语音识别、对话系统等，表示成序列后，其长度事先并不知道。因此如何突破先前深度神经网络的局限，使其可以适应这些场景，成为了2013年以来的研究热点，Seq2Seq框架应运而生。
    
    
## 1.3 Seq2Seq 模型的评价
### 1.3.1 优势：
1. 端到端学习<p>
  Seq2Seq模型实现了从原始输入数据（如文本、语音等）到期望输出序列的直接映射，无需进行显式的特征提取或工程化步骤。
2. 处理可变长度序列<p>
  模型具备处理输入和输出序列的能力，能够适应不同长度的序列数据。
3. 信息压缩与表示<p>
  编码器通过编码过程将输入序列压缩为一个固定维度的上下文向量，该向量作为输入序列的抽象表示，解码器基于此上下文向量进行解码操作以生成目标输出序列。
4. 可扩展性<p>
  Seq2Seq模型具有良好的模块化特性，能够与卷积神经网络（CNNs）、循环神经网络（RNNs）等神经网络架构无缝集成，以应对更加复杂的数据处理任务和场景。

**小故事讲解：Seq2Seq 模型的优势**
1. 一站式学习体验<p>
  Seq2Seq模型就像一个超级学习机器，它能直接从原始的文本或语音等数据中学习，然后直接输出我们想要的结果，整个过程不需要人工去提取特征或进行复杂的预处理。
2. 灵活应对不同长度<p>
  这个模型非常灵活，不管输入的序列有多长，或者输出的序列需要多长，它都能够适应，就像一个万能的变形金刚，能够伸缩自如。
3. 高效的信息提炼<p>
  想象一下把一大段文字浓缩成一个简短的摘要，编码器就是做这样的事情，它把整个输入序列压缩成一个精华的上下文向量。然后，解码器就像一个作家，根据这个摘要重新创作出一篇完整的文章。
4. 易于扩展和升级：<p>
  Seq2Seq模型就像一个模块化的工具箱，可以轻松地与其他神经网络技术（比如卷积神经网络或循环神经网络）结合，这样一来，它就能处理更加复杂和多样化的任务，就像升级装备后的超级英雄，能力更加强大。
  
### 1.3.2 缺点：
1. 上下文向量信息压缩<p>
  输入序列的全部信息需要被编码成一个固定维度的上下文向量，这导致了信息压缩和信息损失的问题，尤其是细粒度细节的丢失
2. 短期记忆限制<p>
  由于循环神经网络（RNN）的固有特性，Seq2Seq模型在处理长序列时存在短期记忆限制，难以有效捕获和传递长期依赖性。这限制了模型在处理具有长距离时间步依赖的序列时的性能。
3. 暴露偏差（Exposure Bias）<p>
  在Seq2Seq模型的训练过程中，经常采用“teacher forcing”策略，即在每个时间步提供真实的输出作为解码器的输入。然而，这种训练模式与模型在推理时的自回归生成模式存在不一致性，导致模型在测试时可能无法很好地适应其自身的错误输出，这种现象被称为暴露偏差。

**小故事讲解：Seq2Seq 模型的缺点**
1. 信息打包难题<p>
  想象一下，你试图把一本厚厚的百科全书的内容全部塞进一个小小的记忆卡片里。Seq2Seq模型在做类似的事情，它需要把整个输入序列的信息压缩成一个固定大小的上下文向量。这就好比你只能记住百科全书的概要，而丢失了很多细致入微的细节。
2. 记忆挑战<p>
  Seq2Seq模型有时候就像一个有短期记忆障碍的人，它很难回忆起很久以前发生的事情。这意味着，当处理很长的序列时，模型往往难以捕捉到序列开始和结束之间的长期依赖关系，就像试图回忆一个长故事的每一个细节一样困难。
3. 训练与实战的差距<p> 
  在训练过程中，我们常常用一种叫做“teacher forcing”的方法来训练Seq2Seq模型，这就像是考试时老师不断给你提示答案。但是，在实际使用模型时，它需要自己猜测下一步该做什么，这就可能导致模型在实际应用时表现不如训练时那么出色，因为它可能会过分依赖那些训练时的“提示”，而忽略了如何独立解决问题。这种训练和实际应用之间的差异，我们称之为“Exposure Bias”。

# 2. Encoder-Decoder模型
## 2.1  “Seq2Seq”和“Encoder-Decoder”的关系
Seq2Seq 模型（**强调目的**）：不特指具体方法，而是以满足“输入序列、输出序列”的目的。而 Seq2Seq 使用的具体方法基本都属于Encoder-Decoder 模型（**强调方法**）的范畴。
总结一下的话：
- Seq2Seq模型是Encoder-Decoder架构的一种具体应用
- Seq2Seq 更强调目的，Encoder-Decoder 更强调方法

**Encoder-Decoder**将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题。
Encoder 又称作编码器。它的作用就是“将现实问题转化为数学问”。
Decoder 又称作解码器，他的作用是“求解数学问题，并转化为现实世界的解决方案”。<p>
关于 Encoder-Decoder，有2点需要说明：
1. 不论输入和输出的长度是什么，中间的“上下文向量 ” 长度都是固定的（这也是它的缺陷，下文会详细说明）
2. 根据不同的任务可以选择不同的编码器和解码器（可以是一个 RNN ，但通常是其变种 LSTM 或者 GRU ）
只要是符合上面的框架，都可以统称为 Encoder-Decoder 模型

## 2.2 Encoder-Decoder 的工作流程
### 2.2.1 编码器（Encoder）
编码器是一个循环神经网络（RNN），由多个循环单元（如 LSTM 或 GRU）堆叠而成。每个单元通过依次处理输入序列中的一个元素，生成一个上下文向量。这个上下文向量汇总了输入序列的全部信息，它通过在单元间传递当前元素的信息，逐步构建对整个序列的理解。
**工作流程：**
1. 词序列转换<p>
  在Seq2Seq模型的初始阶段，输入文本（例如一个句子）通过一个嵌入层（embedding layer）进行转换。这一层将每个词汇映射到一个**高维空间中的稠密向量**，这些向量携带了词汇的语义信息。这个过程称为**词嵌入**（**word embedding**），它使得模型能够捕捉到词汇之间的内在联系和相似性。

2. 序列处理<p>
  随后，这些经过嵌入的向量序列被送入一个基于循环神经网络（RNN）的结构中，该结构可能由普通的RNN单元、长短期记忆网络（LSTM）单元或门控循环单元（GRU）组成。RNN的递归性质允许它处理时序数据，从而捕捉输入序列中的顺序信息和上下文关系。在每个时间步，RNN单元不仅处理当前词汇的向量，还结合了来自前一个时间步的隐藏状态信息。
3. 生成上下文向量<p>
  经过一系列时间步的计算，RNN的**最后一个隐藏层输出**被用作整个输入序列的表示，这个输出被称为“**上下文向量**（**context vector**）”。上下文向量是一个**固定长度**的向量，它通过汇总和压缩整个序列的信息，有效地编码了输入文本的整体语义内容。这个向量随后将作为Decoder端的重要输入，用于生成目标序列。

### 2.2.2 解码器（Decoder）
解码器同样采用递归神经网络（RNN）架构，它接收编码器输出的上下文向量作为其初始输入，并依次合成目标序列的各个元素。
    
**工作流程：**
1. 初始化参数
在开始训练之前，解码器RNN及其相关输出层的参数需要被初始化。这些参数包括：
      - 权重（Weights）：连接不同神经网络层的参数，它们决定了层与层之间信息的传递方式。
      - 偏置（Biases）：在神经网络层中加入的固定值，用于调节激活函数的输出。 <p>
    
参数的初始化通常涉及对它们赋予随机的小数值，以防止网络在训练初期对特定输出产生偏好。常用的初始化策略包括**Xavier初始化**(适用于Sigmoid或Tanh激活函数)和**He初始化**(通常用于ReLU激活函数)，这两种方法旨在维持各层激活值和梯度的大致稳定性，从而优化训练过程。<p>
2. 编码器输出<p>
在Seq2Seq框架中，编码器负责对源序列（如一个句子）进行初步处理。编码器RNN可以是基本RNN、长短期记忆网络（LSTM）或门控循环单元（GRU）。编码器的工作流程如下：
  - 输入序列处理：编码器针对每个输入元素更新其隐藏状态，该状态累积了直至当前元素的序列信息。
  - 隐藏状态更新：对于每个输入元素，编码器更新其隐藏状态，这个隐藏状态捕获了到当前元素为止的序列信息。
  - 上下文向量生成：编码器的最后一个隐藏状态（或在注意力机制中，所有隐藏状态的加权平均）被用作上下文向量。这个向量是源序列的压缩表示，包含了序列的全部或关键信息。
3. 解码器输入<p>
编码器完成上下文向量的生成后，解码器便开始目标序列的合成过程。以下是解码器输入的详细说明：
- (1)初始隐藏状态<p>
  编码器生成的上下文向量通常被设定为解码器的初始隐藏状态。在某些实现中，上下文向量可能会与解码器的第一个隐藏状态合并或拼接。
- (2)开始符号（SOS）<p>
  解码器需要一个触发信号来启动序列的生成。这一信号通常由特定的开始符号（Start-of-Sequence, ）表示，它告诉解码器开始构建目标序列。
- (3)输入序列（Input Sequence）<p>
  在第一个时间步之后，解码器的输入将是前一个时间步的输出。在训练过程中，这可能是真实的目标序列中的下一个词，或者是在预测过程中模型自己的预测。
- (4)上下文向量（Context Vector）<p>
  在每个时间步，解码器可能会接收到一个上下文向量，这个向量是编码器输出的一个函数，它总结了整个源序列的信息。在带有注意力机制的模型中，上下文向量是动态计算的，它反映了当前解码位置对源序列不同部分的关注程度。
- (5)注意力权重（Attention Weights）<p>
  如果模型采用了注意力机制，那么每个时间步解码器还会接收一组注意力权重。这些权重决定了编码器输出中哪些部分对于当前时间步的解码最为重要。
    
    
### 2.2.3 Seq2Seq模型的训练过程
训练Seq2Seq模是一个复杂的过程，为方便你理解，需要你暂时扮演一名老师，教一个学生（解码器）如何根据一本故事书（源序列）来复述故事。这个过程可以这样进行：<p>
**1. 准备数据**
- 数据预处理
  - 首先，你需要把故事书的内容翻译成学生能理解的词汇表（**分词**），并为每个词汇分配一个编号，同时在故事的开始和结束加上特殊的标记（**编码**），告诉学生何时开始和结束复述。
- 批量处理
  - 然后，你把这些故事分成几个小部分，每次给学生讲一个小故事，这样他们可以同时学习多个故事。<p>
    
**2. 初始化模型参数**
- 解码器RNN
  - 学生的大脑就像一个初始状态，需要学习如何理解和复述故事。（**初始化**解码器的权重和偏置。）
- 输出层
  - 学生的嘴巴就像一个输出层，需要学习如何将大脑中的故事转换成语言表达出来。（初始化输出层的权重和偏置，通常是一个全连接层，用于将解码器的隐藏状态**映射**到目标词汇的概率分布。）<p>
    
**3. 编码器处理**
- 运行编码器
  - 你先给学生讲一遍故事，让他们理解故事的情节和要点，这些信息被总结成一个简短的笔记。（通过编码器RNN处理源序列，得到最后一个隐藏状态或使用注意力机制来生成上下文向量。）<p>
    
**4. 解码器训练过程**
- 初始化隐藏状态
  - 学生开始复述前，先看看你给他们的笔记。（使用编码器的最后一个隐藏状态或上下文向量来初始化解码器的隐藏状态。）
- 时间步迭代：
  - 输入
    - 学生首先根据你给的开头标记（<BOS>）开始复述，然后根据他们自己的记忆或者你给的提示（教师强制 Teacher Forcing）来继续。
  - 解码器RNN
    - 学生的大脑根据**当前的输入**和**之前的记忆**来决定下一步该说什么。（根据当前时间步的输入和前一时间步的隐藏状态，计算当前时间步的隐藏状态。）
  - 输出层
    - 学生尝试用语言表达出他们大脑中的故事。（将当前时间步的隐藏状态通过输出层，得到一个**概率分布**，表示当前时间步预测的目标词汇。）
  - 损失计算
    - 你评估学生复述的故事和你讲的故事之间的差异，通常是通过比较他们说的词汇和你期望的词汇。（计算预测的概率分布和实际目标词汇之间的损失，通常使用**交叉熵损失**。）<p>
    
**5. 反向传播和参数更新**
- 计算梯度
  - 学生根据你的反馈来调整他们的记忆和学习方法。（**对损失函数进行反向传播**，计算模型参数的梯度。）
- 参数更新
  - 学生通过不断的练习来改进他们的复述技巧。（根据计算出的梯度，更新解码器的权重和偏置。）<p>
    
**6. 循环训练**
- 多个epoch
  - 学生需要反复练习，多次复述不同的故事，直到他们能够熟练地复述任何故事（**模型收敛**）。
7. 评估和调优：
- 验证集评估
  - 定期让学生在新的故事（**验证集**）上测试他们的复述能力。
- 超参数调优
  - 根据学生的表现，调整教学的方法，比如改变练习的难度或频率（**学习率、批次大小**）。

通过这个过程，学生（解码器）学会了如何根据你提供的笔记（上下文向量）来复述故事（生成目标序列）。在实际训练中，教师强制（Teacher Forcing）等技巧可以帮助学生更快地学习，而梯度裁剪（Gradient Clipping）等技术可以防止他们在学习过程中过度自信或偏离正确的路径。
    
### 2.2.4 小故事讲解
Encoder-Decoder 的工作原理：想象一下，你有一个翻译笔，它能够把一种语言翻译成另一种语言。
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%20(3).png)
    
编码器（Encoder）的工作： 
- 这个翻译笔的第一步是“阅读”你输入的句子。它一个词一个词地看，一边看一边记下重要的信息。当它看完整个句子后，它会把所有重要的信息总结成一个简短的笔记，这个笔记就是我们说的“**上下文向量**”。
    
解码器（Decoder）的工作：
- 接下来，翻译笔开始根据这个句子来生成新的句子。它从第一个词开始，可能是“**开始**”的标志，然后根据这个标志和笔记来猜测下一个词是什么。每猜对一个词，它就继续猜下一个，直到它觉得句子翻译完成了，可能是遇到了“**结束**”的标志。<p>
    
这个过程就像是一个人根据记忆来复述一个故事，一边回忆一边讲述，直到故事讲完。解码器（Decoder）每次都是根据它之前说的词和那个总结的笔记（上下文向量）来决定接下来该说什么。

## 2.3 Encoder-Decoder 的应用
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%20(4).png)
    
**机器翻译、对话机器人、诗词生成、代码补全、文章摘要（文本 – 文本）**<p>
“文本 – 文本” 是最典型的应用，其输入序列和输出序列的长度可能会有较大的差异。<p>
Google 发表的用Seq2Seq做机器翻译的论文《Sequence to Sequence Learning with Neural Networks》
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%20(5).png)

**语音识别（音频 – 文本）**<p>
语音识别也有很强的序列特征，比较适合 Encoder-Decoder 模型。<p>
Google 发表的使用Seq2Seq做语音识别的论文《A Comparison of Sequence-to-Sequence Models for Speech Recognition》
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%20(6).png)

**图像描述生成（图片 – 文本）**<p>
通俗的讲就是“看图说话”，机器提取图片特征，然后用文字表达出来。这个应用是计算机视觉和 NLP 的结合。<p>
图像描述生成的论文《Sequence to Sequence – Video to Text》
  
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%20(7).png)


## 2.4 Encoder-Decoder 的缺陷
上文提到：Encoder（编码器）和 Decoder（解码器）之间只有一个固定长度的“上下文向量 ”来传递信息。为了便于理解，我们类比为“压缩-解压”的过程：<p>
将一张 800X800 像素的图片压缩成 100KB，看上去还比较清晰。再将一张 3000X3000 像素的图片也压缩到 100KB，看上去就模糊了（尤其是图片中的配文）。
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%EF%BC%888%EF%BC%89.png)
    
Encoder-Decoder 就是面临类似的问题：当输入信息太长时，会丢失掉一些信息。
    
    
# 3. Attention 的提出与影响
## 3.1 Attention的发展历程
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image(9).png)

| 时间 | 发展 | 描述 |
|------|------|------|
| 早期 | 循环神经网络（RNNs）<br><br>长短期记忆网络（LSTMs）| RNNs能够处理变长的序列数据，但由于梯度消失和梯度爆炸问题，它们在长序列上的表现并不理想。<br> <br>LSTMs是对RNNs的改进，通过引入门控机制来解决长序列学习中的梯度消失问题。|
| 2014年 | Seq2Seq模型的提出<br>  | Seq2Seq模型由一个编码器和一个解码器组成，两者都是基于LSTM。该模型在机器翻译任务上取得了显著的成功。<br><br>**历史地位**:<br>  - 首次引入了编码器-解码器框架;<br>- 端到端的学习成为可能;<br>- 中间产生了上下文向量，把编码过程和解码过程进行了解耦。 |
| 2015年 | 注意力机制的引入| 在Seq2Seq模型的基础上，Bahdanau等人提出了带注意力机制的Seq2Seq模型。|
| 2017年 | 自注意力与Transformer模型 | Transformer模型完全基于自注意力机制，摒弃了传统的循环网络结构，在处理长距离依赖方面表现卓越。 |
| 2018年 | 多头注意力与BERT | Transformer模型进一步发展为多头注意力机制，并在BERT模型中得到应用，BERT在多项NLP任务上取得了突破性的成果。 |

    
## 3.2 Attention的N种类型
从计算区域、所用信息、使用模型、权值计算方式和模型结构方面对Attention的形式进行归类，如下表所示：

| 计算区域 | 所用信息 | 使用模型 |权值计算方式|模型结构|
|------|------|------|---|----|
|1. Soft Attention(Global Attention) | 1.General Attention| 1. CNN + Attention|1.点乘算法|1. One-Head Attention|
|2. Local Attention|2. Self Attention|2. RNN + Attention|2. 矩阵相乘|2. Mutil-layer Attention|
|3. Hard Attention||3. LSTM + Attention|3.Cos相似度|3.Mutil-head Attention|
|       |       |4. pure-Attention|4. 串联方式|
||||5. 多层感知|
    
## 3.2 Attention 解决信息丢失问题
Ａttention 模型的特点是 Encoder 不再将整个输入序列编码为固定长度的“向量C” ，而是编码成一个向量（Context vector）的序列（“C1”、“C2”、“C3”），解决“信息过长，信息丢失”的问题。<p>
引入了 Ａttention 的 Encoder-Decoder 模型如下图：
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%EF%BC%8810%EF%BC%89.png)
    
这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。
    
## 3.3 Attention 的核心工作
Attention 的核心工作就是“**关注重点**”。在特定场景下，解决特定问题。
- 场景：信息量大，包含有效信息和噪声。
- 问题：大脑算力和资源有限，无法同时处理所有信息。
- 应对：快速从繁杂信息中检索出对解决问题最重要的信息。

这很像人类看图片的逻辑，当我们看下面这张熊猫图片的时候，我们将注意力集中在了图片的焦点上（熊猫）。
1. 信息：整个画面
2. 有效信息：画面中心位置
3. 无效信息：画面的边边角角、底色、花纹等等
    
![images](https://github.com/Spr1ng7/fun-transformer/blob/main/docs/chapter1/images/image%20(11).png)
    
在图片上，我们的视线聚焦在**熊猫**及其攀爬的**树干**，熊猫和树干在这个区域内非常清晰，在此之外的区域则相对模糊，表示其他景物（例如熊猫背后的山林）没有被“注意力”所关注。这样的图像就能够直观地展示注意力机制是如何在众多信息中挑选出关键部分进行重点处理的。
如上面所说的，我们的视觉系统就是一种 Attention机制，**将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。**
    
## 3.4 Attention 的3大优点
之所以要引入 Attention 机制，主要是3个原因：
1. **参数少**：模型复杂度跟 CNN、RNN 相比，复杂度更小，参数也更少。所以对算力的要求也就更小。
2. **速度快**：Attention 解决了 RNN 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。
3. **效果好**：在 Attention 机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。

# 4. 参考链接
1、https://huggingface.co/docs/transformers/model_doc/vit

2、Attention用于NLP的一些小结：https://zhuanlan.zhihu.com/p/35739040

3、遍地开花的 Attention，你真的懂吗？：https://zhuanlan.zhihu.com/p/77307258

4、熊猫图片来源：http://www.dili360.com/article/p5e8187a77f0c415.htm

5、https://tech.dewu.com/article?id=109

6、B站李沐老师关于深度学习讲解：https://space.bilibili.com/1567748478
