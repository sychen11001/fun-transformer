{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a26c77",
   "metadata": {},
   "source": [
    "使用了transformers库中的BertModel模型。\n",
    "\n",
    "输出结果是一个包含多个字段的BaseModelOutputWithPoolingAndCrossAttentions对象，这些字段代表了模型处理输入后的不同输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86010ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0781,  0.1587,  0.0400,  ..., -0.2805,  0.0248,  0.4081],\n",
      "         [-0.2016,  0.1781,  0.4184,  ..., -0.2522,  0.3630, -0.0979],\n",
      "         [-0.7156,  0.6751,  0.6017,  ..., -1.1032,  0.0797,  0.0567],\n",
      "         [ 0.0527, -0.1483,  1.3609,  ..., -0.4513,  0.1274,  0.2655],\n",
      "         [-0.7122, -0.4815, -0.1438,  ...,  0.5602, -0.1062, -0.1301],\n",
      "         [ 0.9955,  0.1328, -0.0621,  ...,  0.2460, -0.6502, -0.3296]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8130, -0.2470, -0.7289,  0.5582,  0.3357, -0.0758,  0.7851,  0.1526,\n",
      "         -0.5705, -0.9997, -0.3183,  0.7643,  0.9550,  0.5801,  0.9046, -0.6037,\n",
      "         -0.3113, -0.5445,  0.3740, -0.4197,  0.5471,  0.9996,  0.0560,  0.2710,\n",
      "          0.3869,  0.9316, -0.7260,  0.8900,  0.9311,  0.5901, -0.5208,  0.0532,\n",
      "         -0.9711, -0.1791, -0.8414, -0.9663,  0.2318, -0.6239,  0.0885,  0.1203,\n",
      "         -0.8333,  0.1662,  0.9993,  0.1384,  0.1207, -0.3476, -1.0000,  0.2947,\n",
      "         -0.7443,  0.7037,  0.6979,  0.5853,  0.0875,  0.4013,  0.3722,  0.1009,\n",
      "         -0.1470,  0.1421, -0.2055, -0.4406, -0.6010,  0.2476, -0.7887, -0.8612,\n",
      "          0.8639,  0.7504, -0.0738, -0.2541,  0.0941, -0.1272,  0.7828,  0.1683,\n",
      "          0.0685, -0.8279,  0.4741,  0.2687, -0.6123,  1.0000, -0.3837, -0.9341,\n",
      "          0.5166,  0.5990,  0.5714, -0.2885,  0.4897, -1.0000,  0.2800, -0.1625,\n",
      "         -0.9728,  0.2292,  0.3729, -0.1447,  0.2490,  0.5224, -0.5050, -0.3634,\n",
      "         -0.2048, -0.7688, -0.2677, -0.1745, -0.0355, -0.2574, -0.1838, -0.3517,\n",
      "          0.2785, -0.3823, -0.3204,  0.4208, -0.0671,  0.6005,  0.3758, -0.3386,\n",
      "          0.4421, -0.9251,  0.5425, -0.2365, -0.9684, -0.5510, -0.9714,  0.4726,\n",
      "         -0.2355, -0.3178,  0.8958,  0.1285,  0.2222,  0.0103, -0.5784, -1.0000,\n",
      "         -0.5691, -0.5153, -0.0901, -0.1982, -0.9424, -0.9055,  0.4781,  0.9141,\n",
      "          0.0904,  0.9976, -0.2006,  0.8990, -0.3713, -0.6045,  0.5630, -0.3681,\n",
      "          0.7174,  0.1177, -0.4574,  0.1722, -0.0565,  0.2068, -0.5352, -0.1658,\n",
      "         -0.6867, -0.8909, -0.3266,  0.9221, -0.3681, -0.8595, -0.1558, -0.1426,\n",
      "         -0.4466,  0.7497,  0.5588,  0.3189, -0.2639,  0.3429,  0.1234,  0.4654,\n",
      "         -0.6938,  0.0370,  0.2499, -0.3322, -0.6972, -0.9597, -0.3168,  0.3914,\n",
      "          0.9778,  0.6148,  0.2344,  0.5841, -0.1060,  0.6453, -0.9023,  0.9467,\n",
      "         -0.2828,  0.2772, -0.2568,  0.3116, -0.7656,  0.2149,  0.7538, -0.4707,\n",
      "         -0.7183,  0.0423, -0.3983, -0.2417, -0.6220,  0.3196, -0.2374, -0.2951,\n",
      "          0.0163,  0.8490,  0.9524,  0.6414,  0.1112,  0.5715, -0.7747, -0.4595,\n",
      "         -0.0131,  0.1861,  0.2833,  0.9822, -0.5268, -0.0747, -0.8972, -0.9714,\n",
      "         -0.1566, -0.8502, -0.1051, -0.6196,  0.5066,  0.1431,  0.4483,  0.3532,\n",
      "         -0.9523, -0.6036,  0.2748, -0.2628,  0.3665, -0.1634,  0.2737,  0.8655,\n",
      "         -0.3408,  0.4985,  0.8835, -0.7243, -0.7103,  0.7471, -0.1439,  0.7210,\n",
      "         -0.4727,  0.9464,  0.7754,  0.6428, -0.8258, -0.5042, -0.7309, -0.5623,\n",
      "          0.0212,  0.0334,  0.8216,  0.5628,  0.2801,  0.3605, -0.5478,  0.9828,\n",
      "         -0.3884, -0.9035, -0.2619, -0.0374, -0.9656,  0.6856,  0.3733,  0.0614,\n",
      "         -0.3911, -0.5584, -0.9042,  0.8116,  0.0722,  0.9598,  0.0835, -0.8356,\n",
      "         -0.5096, -0.8548, -0.2696, -0.1833, -0.3283, -0.1286, -0.9169,  0.3966,\n",
      "          0.3708,  0.4118, -0.5972,  0.9906,  1.0000,  0.9055,  0.8319,  0.7821,\n",
      "         -0.9966, -0.4511,  0.9999, -0.9403, -1.0000, -0.8683, -0.5315,  0.2191,\n",
      "         -1.0000, -0.2363,  0.1403, -0.8282,  0.4823,  0.9450,  0.9512, -1.0000,\n",
      "          0.5255,  0.8806, -0.6060,  0.8864, -0.2879,  0.9407,  0.5752,  0.0417,\n",
      "         -0.1602,  0.2897, -0.8608, -0.7736, -0.3691, -0.5601,  0.9836,  0.1345,\n",
      "         -0.7325, -0.8195,  0.0158, -0.1550, -0.3395, -0.9400, -0.0907,  0.4092,\n",
      "          0.6562,  0.0334,  0.2541, -0.5023,  0.2764,  0.0445,  0.0117,  0.6056,\n",
      "         -0.8882, -0.4208, -0.4594, -0.2406, -0.4968, -0.9285,  0.9085, -0.4580,\n",
      "          0.7120,  1.0000,  0.3074, -0.7186,  0.4775,  0.1430, -0.2709,  1.0000,\n",
      "          0.7428, -0.9434, -0.4836,  0.4347, -0.3803, -0.4038,  0.9934, -0.2347,\n",
      "         -0.5286, -0.1938,  0.9469, -0.9670,  0.9769, -0.7885, -0.9335,  0.9128,\n",
      "          0.8891, -0.5778, -0.3787,  0.0850, -0.6286,  0.2903, -0.9003,  0.5381,\n",
      "          0.3541,  0.0589,  0.8031, -0.7100, -0.5081,  0.1781, -0.6506, -0.2105,\n",
      "          0.6789,  0.4671, -0.3476,  0.0115, -0.3682, -0.0806, -0.9431,  0.3343,\n",
      "          1.0000, -0.1909,  0.5555, -0.3358,  0.0263, -0.1470,  0.4349,  0.5208,\n",
      "         -0.2424, -0.7325,  0.7086, -0.9245, -0.9652,  0.7596,  0.1530, -0.1946,\n",
      "          0.9997,  0.4134,  0.1137,  0.2351,  0.9249, -0.0421,  0.4099,  0.8266,\n",
      "          0.9554, -0.1834,  0.5092,  0.6651, -0.7831, -0.2729, -0.5306, -0.0974,\n",
      "         -0.8989,  0.1261, -0.9079,  0.9355,  0.7817,  0.3302,  0.1569,  0.5768,\n",
      "          1.0000, -0.4562,  0.6240, -0.0049,  0.7729, -0.9976, -0.7388, -0.2276,\n",
      "          0.0201, -0.6561, -0.2721,  0.2316, -0.9417,  0.7118,  0.4029, -0.9698,\n",
      "         -0.9794, -0.2359,  0.7496,  0.0040, -0.9167, -0.5806, -0.4866,  0.4962,\n",
      "         -0.1977, -0.9085, -0.0888, -0.2179,  0.3301, -0.0793,  0.5415,  0.7117,\n",
      "          0.6331, -0.2443,  0.0208, -0.0152, -0.7002,  0.7866, -0.6876, -0.7593,\n",
      "         -0.1856,  1.0000, -0.5540,  0.7209,  0.6980,  0.5925, -0.0925,  0.1437,\n",
      "          0.8451,  0.2209, -0.6994, -0.7470, -0.5233, -0.2470,  0.5377,  0.0122,\n",
      "          0.6115,  0.6328,  0.5893,  0.0072,  0.0707, -0.1691,  0.9949, -0.0650,\n",
      "          0.0342, -0.3035,  0.0180, -0.2952, -0.2453,  1.0000,  0.2287,  0.3614,\n",
      "         -0.9741, -0.6818, -0.8467,  1.0000,  0.7774, -0.4980,  0.5307,  0.6857,\n",
      "         -0.0855,  0.7175, -0.1386, -0.3548,  0.2636,  0.0658,  0.8976, -0.5248,\n",
      "         -0.9286, -0.4457,  0.3075, -0.9184,  0.9985, -0.3725, -0.1925, -0.3857,\n",
      "          0.3529,  0.0612, -0.1111, -0.9667,  0.0296,  0.2127,  0.9023,  0.2146,\n",
      "         -0.5105, -0.8216,  0.6384,  0.5562, -0.7653, -0.9036,  0.9096, -0.9726,\n",
      "          0.6010,  1.0000,  0.3428, -0.2678, -0.0040, -0.2032,  0.1948, -0.2583,\n",
      "          0.6594, -0.8841, -0.3110, -0.1139,  0.2070, -0.0499,  0.0766,  0.6247,\n",
      "          0.1220, -0.4255, -0.4315,  0.0340,  0.3513,  0.7545, -0.2616, -0.1054,\n",
      "         -0.0310, -0.0160, -0.8557, -0.2032, -0.2896, -0.9998,  0.4847, -1.0000,\n",
      "          0.1944,  0.0238, -0.1356,  0.7747,  0.0479,  0.4497, -0.6311, -0.7206,\n",
      "          0.6178,  0.6761, -0.2757, -0.3905, -0.5928,  0.2279,  0.0707,  0.1233,\n",
      "         -0.5408,  0.7444, -0.1296,  1.0000,  0.0961, -0.6621, -0.9196,  0.0704,\n",
      "         -0.2162,  1.0000, -0.7794, -0.8807,  0.2784, -0.5381, -0.7780,  0.3047,\n",
      "         -0.0338, -0.6482, -0.7542,  0.8950,  0.8146, -0.5588,  0.3988, -0.2965,\n",
      "         -0.4299,  0.0082,  0.6164,  0.9624,  0.2798,  0.7427,  0.3798, -0.1202,\n",
      "          0.9364,  0.1309,  0.2478,  0.0943,  1.0000,  0.3100, -0.8320,  0.1281,\n",
      "         -0.9242, -0.1620, -0.8809,  0.1841,  0.2992,  0.8720, -0.2030,  0.8995,\n",
      "         -0.4650, -0.0161, -0.6945, -0.4291,  0.2841, -0.8518, -0.9624, -0.9653,\n",
      "          0.6546, -0.4158,  0.0047,  0.1431, -0.0799,  0.2424,  0.3504, -1.0000,\n",
      "          0.9038,  0.3259,  0.8399,  0.8938,  0.6032,  0.4055,  0.2465, -0.9604,\n",
      "         -0.9200, -0.1999, -0.2206,  0.5958,  0.5572,  0.8410,  0.2918, -0.5227,\n",
      "          0.0078, -0.3019, -0.4921, -0.9816,  0.4371, -0.5117, -0.8819,  0.9265,\n",
      "         -0.1755, -0.0597, -0.0823, -0.6530,  0.8747,  0.6324,  0.1266,  0.0842,\n",
      "          0.3756,  0.7381,  0.8790,  0.9607, -0.6325,  0.6952, -0.3407,  0.3745,\n",
      "          0.6012, -0.8744,  0.0708,  0.1280, -0.2589,  0.1242, -0.1201, -0.9274,\n",
      "          0.5940, -0.1563,  0.4428, -0.3287,  0.0695, -0.3981, -0.0400, -0.5623,\n",
      "         -0.6015,  0.5620,  0.2243,  0.8030,  0.7663, -0.0771, -0.4842, -0.1473,\n",
      "         -0.6306, -0.8541,  0.8142,  0.1151, -0.3174,  0.4548, -0.1950,  0.7887,\n",
      "          0.0674, -0.2464, -0.3487, -0.4228,  0.6947,  0.0686, -0.4925, -0.5855,\n",
      "          0.5164,  0.2094,  0.9996, -0.5488, -0.7718, -0.0737, -0.2518,  0.3538,\n",
      "         -0.3599, -1.0000,  0.3665, -0.2367,  0.6221, -0.5721,  0.3542, -0.5887,\n",
      "         -0.9486, -0.2115,  0.1483,  0.6009, -0.4153, -0.6647,  0.4821, -0.1477,\n",
      "          0.8825,  0.7133, -0.2224,  0.2536,  0.5956, -0.6499, -0.6185,  0.8514]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d6be5",
   "metadata": {},
   "source": [
    "# 输出结果中各个字段的解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43e1a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**last_hidden_state**： 这是一个张量，代表了模型处理输入后的最后一个隐藏状态。这个隐藏状态通常用于进一步的处理，如分类、序列到序列的任务等。对于BertModel，这个隐藏状态是在模型中最后一个Transformer层之后的输出。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**pooler_output**: 这也是一个张量，代表了模型在最后一个Transformer层之后的输出，经过一个线性层和Tanh激活函数的输出。这个输出通常用于分类任务。\n",
    "\n",
    "\n",
    "**hidden_states**: 这个字段是一个包含多个隐藏状态的列表，每个隐藏状态对应模型中的一个Transformer层。这些隐藏状态可以用于可视化模型的内部工作方式或进一步分析。\n",
    "\n",
    "\n",
    "**past_key_values**: 这个字段是模型在序列到序列任务中使用的，用于存储前一个序列的注意力权重和隐藏状态，以便在下一个序列中重用。\n",
    "    \n",
    "    \n",
    "**attentions**: 这个字段是一个包含多个注意力权重张量的列表，每个注意力权重张量对应模型中的一个Transformer层。这些注意力权重可以用于可视化模型如何关注输入序列的不同部分。\n",
    "\n",
    "\n",
    "**cross_attentions**: 这个字段是模型在序列到序列任务中使用的，用于存储不同序列之间的注意力权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef92cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
